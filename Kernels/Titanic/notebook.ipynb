{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "0f2e1b48-de36-4da8-9df9-a2909a92b461",
        "_uuid": "721884fd77000b8b4e69c2af9958f4412afd2cff"
      },
      "cell_type": "markdown",
      "source": "# Predicting Titanic Survivers\nLike Titanic, this is my maiden voyage,  when it comes to Kaggle contest that is!. I've completed the Data Science track on Data Camp, but I'm a relative newbie when it comes to machine learning. I'm going to attempt to work my way through the Titanic: Machine Learning contest. My aim is to submission and initial entry as quickly as possible to get a base line score and then attempt to improve on  on it by first looking at missing data, then engineering key features before establishing a  secondary base line and trying to improve the model itself. \n\nPlease feel free to post comments or  make suggestions as to what i may be doing wrong or could maybe do better and  consider upvoting if you find the notebook useful!"
    },
    {
      "metadata": {
        "_cell_guid": "bb27af35-206d-4da2-8f33-63a6b6671c72",
        "_uuid": "13a824268233d8a6a7002be362847e2446aa2da6"
      },
      "cell_type": "markdown",
      "source": "# Import the Libraries and Data"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "collapsed": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\ndf_train=pd.read_csv('../input/train.csv',sep=',')\ndf_test=pd.read_csv('../input/test.csv',sep=',')\n\nPassengerId = df_test['PassengerId']\nSubmission=pd.DataFrame()\nSubmission['PassengerId'] = df_test['PassengerId']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "02b0dfd2-d87f-4579-a427-bd1b2e528412",
        "_uuid": "967244f872d968b22dc6dbae5916895c4c07b781"
      },
      "cell_type": "markdown",
      "source": "# Stage 1 : Explore the Data and create a basic model on raw data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Explore the data Statistically"
    },
    {
      "metadata": {
        "_cell_guid": "073e7a38-5e62-427e-ba8d-72a7a5334442",
        "collapsed": true,
        "_uuid": "47cafc06d2881a01e80469d40ed38cbea126652c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# How big are the training and test datasets\nprint(df_train.shape)\nprint(\"----------------------------\")\nprint(df_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4be833c5-e99c-4519-9b29-4d27699cfe12",
        "collapsed": true,
        "_uuid": "ec6fdc9350bc1e3ce83114c370008528e8109afa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What are the column names \nprint(df_train.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1e27d5dd-1d67-438f-87c3-ea4577f297cd",
        "collapsed": true,
        "_uuid": "4534fe968fece75089f136f647d5ee9f1408b21e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What type of data object are in each column and how many missing values are there\nprint(df_train.info())\nprint(\"----------------------------\")\nprint(df_test.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f9bee28e-9b27-4f2b-8bf5-8e7a2b09ebbf",
        "collapsed": true,
        "_uuid": "77c81d38447b990a95f21afe3fae8bb5f6d00b4e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#check for any other unusable values\nprint(pd.isnull(df_train).sum())\nprint(\"----------------------------\")\nprint(pd.isnull(df_test).sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "11c1e7bb-6f12-48ca-8807-add46fb447f4",
        "_uuid": "5bf733a2fc59ab5fd469aa07d107c06ffddc50a2"
      },
      "cell_type": "markdown",
      "source": "## Observations on missing data.\n\nThere are 144 missing ages in the training data and 86 mssing ages in the test data. Age is an important feature so it is worth spending time to address this properly. \n\nThere are 468 missing Cabin entries in the training data and 326 in the test data, at this stage I'm not sure how important this feature is so I'm going to revisit this when I know more about the feature.\nThere are 2 missing embarked data points in the train data and 1 missing fare in the test data, at this stage this does not represent a problem."
    },
    {
      "metadata": {
        "_cell_guid": "817d81d9-6912-4771-b654-b3d5af85ed46",
        "collapsed": true,
        "_uuid": "10dbf2b79a79bbdcb283fb8237cf9a7eb94bccb2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Get a statistical overview of the data\nprint(df_train.describe())\nprint(\"----------------------------\")\nprint(df_test.describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "37d3c26f-7b3e-411d-b939-ad874ba31eea",
        "collapsed": true,
        "_uuid": "ec577a60cb08406719037e966a974f29573b57fb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Take a look at some sample data\nprint(df_train.head(10))\nprint(df_train.tail(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2275818c-6652-4070-be61-b17a7e4b0b5e",
        "_uuid": "76fbda700a5f261b31759ade3e8c2c93d7458c01"
      },
      "cell_type": "markdown",
      "source": "# Explore Data Graphically"
    },
    {
      "metadata": {
        "_cell_guid": "f001f636-fc2a-4881-a81e-53edabf53242",
        "_uuid": "fb00c6099b7f836f3610e911075f78af6ca724f2"
      },
      "cell_type": "markdown",
      "source": "# Pairplots\n\nTo get a very basic idea of the relationships between the different features we can use pairplots from seaborn."
    },
    {
      "metadata": {
        "_cell_guid": "60f3f81f-4401-4659-b457-4200cabdf2b5",
        "collapsed": true,
        "_uuid": "b81a66434766fce1abfc6ad160ec6a62bee97887",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "g = sns.pairplot(df_train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked']], hue='Survived', palette = 'seismic',size=4,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=50) )\ng.set(xticklabels=[])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1de34739-550f-4384-a3e0-0ea807bb297e",
        "_uuid": "ab4504d0c13948980ed23f80d155fd7d31cd4301"
      },
      "cell_type": "markdown",
      "source": "# Create simple model\n\nCreate a baseline score by using old the standard numeric data on on a very basic model, this will be used to see how much any changes we make to the data or model improve performance."
    },
    {
      "metadata": {
        "_cell_guid": "dd1b861c-f4d7-4d2f-a131-9b6f59b48624",
        "collapsed": true,
        "_uuid": "ca99023e73866ccccf2b58dadd69242defaea90c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Fare']\nOTHER_COLUMNS=['Sex', 'Embarked','Name','Ticket','Cabin']\n\n# create test and training data\ndata_to_train = df_train[NUMERIC_COLUMNS].fillna(-1000)\ny=df_train['Survived']\nX=data_to_train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=21, stratify=y)\n\nclf = SVC()\n# clf = OneVsRestClassifier(LogisticRegression())\nclf.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5c53d58a-aff8-4086-834a-2d4b83544433",
        "_uuid": "5b1d48196352ab9a88c1d6d3e9588dc3846a7653"
      },
      "cell_type": "markdown",
      "source": "# Check the Accuracy of model on test data\n"
    },
    {
      "metadata": {
        "_cell_guid": "a17c324b-c9cf-4fb4-99ac-1b0d781f2d17",
        "collapsed": true,
        "_uuid": "04ec55db09642f8fd1df74b2b5fc5ba68dbab0ae",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Print the accuracy\n\nprint(\"Accuracy: {}\".format(clf.score(X_test, y_test)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "23cb1c05-31ae-4676-934e-62c0e3f53821",
        "_uuid": "a9a81d694ac9b10f1558318ca64ec38d689411e0"
      },
      "cell_type": "markdown",
      "source": "# Create initial predictionsÂ¶"
    },
    {
      "metadata": {
        "_cell_guid": "edb5f39c-2c88-4c42-9bde-f6375503cef3",
        "collapsed": true,
        "_uuid": "50e1643054e16b31c2234ba18df0176ae4c44696",
        "trusted": false
      },
      "cell_type": "code",
      "source": "test = df_test[NUMERIC_COLUMNS].fillna(-1000)\n\nSubmission['Survived']=clf.predict(test)\nprint(Submission.head(5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "262a3df0-1a62-40bb-be9c-48bbc2787847",
        "_uuid": "df612c7f8c676da096cf50c3640b882a9443d330"
      },
      "cell_type": "markdown",
      "source": "# Make first Submission"
    },
    {
      "metadata": {
        "_cell_guid": "3ab17c71-9324-48ee-815f-e3a92f9cd074",
        "_kg_hide-output": true,
        "_uuid": "5b2bf28e6ab4b43d8368df9606f2d3d18d1bd4ab",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\nSubmission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('myfirstsubmission.csv',sep=',')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f66f0981-d9dd-4c9f-ac7a-f63fddb16dd4",
        "_uuid": "b50c6f50f7d672349871e631132d9f941ba1a02f"
      },
      "cell_type": "markdown",
      "source": "The result of this first submission was a score of 0.57894. This constitutes performing just above random, if i'd simply flipped a coin fair coin for each passenger i could have achieved this kind of score. So there is plenty of room for improvement."
    },
    {
      "metadata": {
        "_cell_guid": "3a0ea6c3-2148-408e-b1eb-12b9b4f5cf6b",
        "_uuid": "c4c7faeb87fbe3324c324a01a0dc9bac3e31829d"
      },
      "cell_type": "markdown",
      "source": "# Stage 2 : Clean Data & Engineer features to improve results\n\nHere I am going to go with the principal that those that were in the lifeboats were more likely to survive and from history we know that the women and children were given priority for the life boat places. So i am going to try and engineer features to help the model find the women and children. I've also going to use these features to further explore the data statistically and visually to see if there are further patterns in the data that will help identify additional patterns that might explain anomolies to this, from the initial visualisations it appearred that class might also play a major role in whether certain groups of passengers survived or not.    "
    },
    {
      "metadata": {
        "_cell_guid": "caba1945-2f05-48fa-ba53-adf54b0f89b2",
        "_uuid": "89eaf50cba713ec9fba5a018f36cfdf59f5bdbde"
      },
      "cell_type": "markdown",
      "source": "# Feature Reduction\n\nThe PassengerId and ticket are not really relevant for the training data, so we can drop those elements."
    },
    {
      "metadata": {
        "_cell_guid": "55d0b4df-14fc-49a9-8929-1377726eeaef",
        "collapsed": true,
        "_uuid": "2661dd168bc2c6ff89f2c694cdc3cdd171816714",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Feature reduction\ndrop_elements = ['Ticket','Cabin']\ndf_train = df_train.drop(drop_elements, axis = 1)\ndf_test = df_test.drop(drop_elements, axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5fb4b386-5579-4d42-a396-592f3b3965cd",
        "_uuid": "a2cff155acb305e954ec2635b045bba7fa1cc990"
      },
      "cell_type": "markdown",
      "source": "# Filling in the blanks"
    },
    {
      "metadata": {
        "_cell_guid": "3c0ca4fc-56c8-49bd-bd47-e10c5d4cf5ca",
        "_uuid": "64dffa9832dfab7007e282611fcd4498bf1a1658"
      },
      "cell_type": "markdown",
      "source": "## Estimate missing Fare Data"
    },
    {
      "metadata": {
        "_cell_guid": "a3922967-88cf-455e-9af7-625453f304ee",
        "_uuid": "b877c37918e262367e47d571679499c227645f41",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Fill the na values in Fare\ndf_train[\"Fare\"].fillna(np.median(df_train[\"Fare\"]))\ndf_test[\"Fare\"].fillna(np.median(df_test[\"Fare\"]))\n\n#Create new variable called log_fare, because Fare distribution is VERY skewed.  \ndf_train[\"log_fare\"] = np.log(df_train[\"Fare\"])\ndf_test[\"log_fare\"] = np.log(df_test[\"Fare\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8cf4c617-baf1-4172-85e0-7ae1754e23a8",
        "_uuid": "fad3c30cc68e471b84be592166cc2a661c4a357f"
      },
      "cell_type": "markdown",
      "source": "## Estimate missing Age Data"
    },
    {
      "metadata": {
        "_cell_guid": "331ba67a-59db-45a1-a146-65b752a2c198",
        "collapsed": true,
        "_uuid": "a4ee069224a6668e2e73caaebe2d8d5004fd7503",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Fill the missing Age values\n\n# Age \nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# get average, std, and number of NaN values in titanic_df\naverage_age_titanic   = df_train[\"Age\"].mean()\nstd_age_titanic       = df_train[\"Age\"].std()\ncount_nan_age_titanic = df_train[\"Age\"].isnull().sum()\n\n# get average, std, and number of NaN values in test_df\naverage_age_test   = df_test[\"Age\"].mean()\nstd_age_test       = df_test[\"Age\"].std()\ncount_nan_age_test = df_test[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)\nrand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndf_train['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# fill NaN values in Age column with random values generated\ndf_train[\"Age\"][np.isnan(df_train[\"Age\"])] = rand_1\ndf_test[\"Age\"][np.isnan(df_test[\"Age\"])] = rand_2\n\n# convert from float to int\ndf_train['Age'] = df_train['Age'].astype(int)\ndf_test['Age']    = df_test['Age'].astype(int)\n        \n# plot new Age Values\ndf_train['Age'].hist(bins=70, ax=axis2)\n#df_test['Age'].hist(bins=70, ax=axis4)\n\n# peaks for survived/not survived passengers by their age\nfacet = sns.FacetGrid(df_train, hue=\"Survived\",palette = 'seismic',aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, df_train['Age'].max()))\nfacet.add_legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f8b580fd-9b71-4f2c-8681-195789fb0be7",
        "_uuid": "3d7d900ea6779c8e24e2d4632fb8175b6b2f4f64"
      },
      "cell_type": "markdown",
      "source": "# Feature Engineering\nThis is based on info from 'introduction-to-ensembling-stacking'.\n\nConvert male/female Categories to Columns for training data\nOnce we have Category data, the next stage is to make each category into a column, to do this we use the panda's method get_dummies and use the arguent prefixsep='' to determine what is used in the naming convention on the new columns.\n\n>Example : df.Sex.astype('category')"
    },
    {
      "metadata": {
        "_cell_guid": "42133e77-1cd1-416d-9100-7479a80b1d89",
        "_uuid": "873300197bc688e1350aaf8fe230ad8c57d1c222"
      },
      "cell_type": "markdown",
      "source": "##  Gender Feature"
    },
    {
      "metadata": {
        "_cell_guid": "23e16cb2-8bf9-4904-b9db-9304368cc857",
        "_uuid": "d1b98799dea36fd35ebd29efdb9cf71b72aa0fae",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# convert categories to Columns\ndummies=pd.get_dummies(df_train[['Sex']], prefix_sep='_') #Gender\ndf_train = pd.concat([df_train, dummies], axis=1) \ntestdummies=pd.get_dummies(df_test[['Sex']], prefix_sep='_') #Gender\ndf_test = pd.concat([df_test, testdummies], axis=1) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "73a1e1f3-0c5e-42cc-8cf8-00cecd7d3721",
        "_uuid": "ba21b6f61bafeb38d67a873ac096c91b734d39b5"
      },
      "cell_type": "markdown",
      "source": "## Title Feature"
    },
    {
      "metadata": {
        "_cell_guid": "a87030f8-df3f-4e9d-ad2a-3605d3b20b0b",
        "_uuid": "37e03baec88e410f0b920fe9e9c77b9ccfdba5d9",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Get titles\ndf_train[\"Title\"] = df_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf_test[\"Title\"] = df_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False) \n\n#Unify common titles. \ndf_train[\"Title\"] = df_train[\"Title\"].replace('Mlle', 'Miss')\ndf_test[\"Title\"] = df_test[\"Title\"].replace('Mlle', 'Miss')\ndf_train[\"Title\"] = df_train[\"Title\"].replace('Master', 'Master')\ndf_test[\"Title\"] = df_test[\"Title\"].replace('Master', 'Master')\ndf_train[\"Title\"] = df_train[\"Title\"].replace(['Mme', 'Dona', 'Ms'], 'Mrs')\ndf_test[\"Title\"] = df_test[\"Title\"].replace(['Mme', 'Dona', 'Ms'], 'Mrs')\ndf_train[\"Title\"] = df_train[\"Title\"].replace(['Jonkheer','Don'],'Mr')\ndf_test[\"Title\"] = df_test[\"Title\"].replace(['Jonkheer','Don'],'Mr')\ndf_train[\"Title\"] = df_train[\"Title\"].replace(['Capt','Major', 'Col','Rev','Dr'], 'Services')\ndf_test[\"Title\"] = df_test[\"Title\"].replace(['Capt', 'Col', 'Rev', 'Dr'], 'Services')\ndf_train[\"Title\"] = df_train[\"Title\"].replace(['Lady', 'Countess','Sir'], 'Titled')\ndf_test[\"Title\"] = df_test[\"Title\"].replace(['Lady', 'Countess','Sir'], 'Titled')\n\n# convert Title categories to Columns\ntitledummies=pd.get_dummies(df_train[['Title']], prefix_sep='_') #Title\ndf_train = pd.concat([df_train, titledummies], axis=1) \nttitledummies=pd.get_dummies(df_test[['Title']], prefix_sep='_') #Title\ndf_test = pd.concat([df_test, ttitledummies], axis=1) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a3671e50-5bd3-4b64-8105-3bc67811431b",
        "_uuid": "1c309fa180b6227cf9e9f83ff7a5328cf966c96d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "df_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9d461f10-edb1-49e3-8621-158d4b558400",
        "_uuid": "557f433d9c42746529a03ea65b14091f9c461eaf"
      },
      "cell_type": "markdown",
      "source": "## Embarked Feature"
    },
    {
      "metadata": {
        "_cell_guid": "b556093a-c68b-45cd-8f29-5fad9d1f944f",
        "collapsed": true,
        "_uuid": "50fc5c2a64c4e5dc6bf01b6246a8d1d4a10ff485",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndf_train['Embarked'] = df_train['Embarked'].map(embarked_mapping)\ndf_test['Embarked'] = df_test['Embarked'].map(embarked_mapping)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0ba8d045-4754-4db4-8c64-d61d66d8131a",
        "collapsed": true,
        "_uuid": "274e341fa1abfd0bc9a9439f802b00e46254c3c8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "df_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "782507a9-f438-4aa3-9df7-f25d9d235ce6",
        "_uuid": "48e117f4ee50e84aa659505bf9507cdd8717a4db"
      },
      "cell_type": "markdown",
      "source": "## Fare feature"
    },
    {
      "metadata": {
        "_cell_guid": "93fa2600-7ba9-43d4-89b3-c5884c1a9f83",
        "_uuid": "af66c8d456e7c7984bb64a71605f67e6f0ccb7fb",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(df_test[\"Fare\"])):\n    if pd.isnull(df_test[\"Fare\"][x]):\n        pclass = df_test[\"Pclass\"][x] #Pclass = 3\n        df_test[\"Fare\"][x] = round(df_train[df_train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ndf_train['FareBand'] = pd.qcut(df_train['Fare'], 4, labels = [1, 2, 3, 4])\ndf_test['FareBand'] = pd.qcut(df_test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ndf_train = df_train.drop(['Fare'], axis = 1)\ndf_test = df_test.drop(['Fare'], axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6b594146-05f2-4721-9ee4-69b48964e6be",
        "collapsed": true,
        "_uuid": "caa8dc0e79f22188466d325301a80331b799407f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "df_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1221771d-4505-4865-8efb-34bd9b643133",
        "_uuid": "f264ea422251e2c02721cd954e2df78c2006ad6a"
      },
      "cell_type": "markdown",
      "source": "# Visualize the new features"
    },
    {
      "metadata": {
        "_cell_guid": "7be75411-ef60-45e4-824f-16fda1bd85f7",
        "collapsed": true,
        "_uuid": "8ab428543550a8ac07e737a0b50a3d929cab7767",
        "trusted": false
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"Sex\", row = \"Pclass\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"Age\", \"log_fare\")\ngrid.add_legend()\ngrid",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "61b873b9-bcf0-472f-a840-b99c82c9eed2",
        "collapsed": true,
        "_uuid": "d6bca402abada9bc5759b667b362a4a10ea3cb6b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "facet = sns.FacetGrid(data = df_train, hue = \"Title\", legend_out=True, size = 5)\nfacet = facet.map(sns.kdeplot, \"Age\")\nfacet.add_legend();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9eb86bf1-2838-4aeb-8d63-21e2ec707dca",
        "_uuid": "70b77d246c62e356a9521f8602c844f8ac59c73b"
      },
      "cell_type": "markdown",
      "source": "# Re-train the model on new features"
    },
    {
      "metadata": {
        "_cell_guid": "aaa2c9cc-234e-455a-bb25-339d5655a286",
        "collapsed": true,
        "_uuid": "560b663f936650b7a6c2c844eac395f5de41dd23",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Re-evaluate factoring in gender of passenger\n\nNUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Sex_female','Sex_male','Title_Master', 'Title_Miss',\n       'Title_Mr', 'Title_Mrs', 'Title_Services','Embarked']\n\n# create test and training data\ndata_to_train = df_train[NUMERIC_COLUMNS].fillna(-1000)\ny=df_train['Survived']\nX=data_to_train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=21, stratify=y)\n\nclf = SVC()\n# clf = OneVsRestClassifier(LogisticRegression())\nclf.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3d086cc7-4d81-4721-bce3-27c534932251",
        "_uuid": "e45d5debdc77e75fab680ebda9534f3eccf24672"
      },
      "cell_type": "markdown",
      "source": "# Re-evaluate the on new features"
    },
    {
      "metadata": {
        "_cell_guid": "46625816-d7d8-4ab8-8188-b476c446a853",
        "_kg_hide-output": true,
        "_uuid": "4ae404d5a825a31e6d559e23290d7b9990565fbf",
        "collapsed": true,
        "_kg_hide-input": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Print the accuracy# Print  \nprint(\"Accuracy: {}\".format(clf.score(X_test, y_test)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1ea361c4-4126-4e9d-b548-ea56ef5ae083",
        "_uuid": "f8195440d641ac964d496f1907bb54f64da5a0be"
      },
      "cell_type": "markdown",
      "source": "# Reforcast predictions based on new features"
    },
    {
      "metadata": {
        "_cell_guid": "755a1493-f4be-40ae-be33-9ee302882a6f",
        "collapsed": true,
        "_uuid": "9fdcd905b7a60429ceb4d62620c388a9faa980a7",
        "trusted": false
      },
      "cell_type": "code",
      "source": "test = df_test[NUMERIC_COLUMNS].fillna(-1000)\nSubmission['Survived']=clf.predict(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0d689a14-d161-407b-9fbe-0545d654da7e",
        "_uuid": "50f117aaa9560e10383d0a4b0d891675c0d328a2"
      },
      "cell_type": "markdown",
      "source": "# Make revised submission"
    },
    {
      "metadata": {
        "_cell_guid": "f61ee101-471d-4425-9a95-a10e1b269bdf",
        "collapsed": true,
        "_uuid": "8971da22e3035e23af2f7b587dcdab60a126d1b9",
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\n#Submission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('revisedsubmission.csv',sep=',')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "68850cd4-c427-4f53-8af6-a7f2c9a9df1b",
        "_uuid": "73fb67722c4b233ff29e491955b752f99eb2a539"
      },
      "cell_type": "markdown",
      "source": "The second revised submission scored 0.75598 which was an improvement of the original revision which scored 0.64593, this used was  is an improvement on the original score of 0.57894. This advanced the submission to 9117 place on the leaderboard, from the starting point of 10599th place! Obviousy a step in the right direction but still needing work."
    },
    {
      "metadata": {
        "_cell_guid": "b4680f97-d9e3-40d8-b5b3-39d914e25965",
        "_uuid": "f3425a2b3b51e62d06f9c7550b485ef25b3c7539"
      },
      "cell_type": "markdown",
      "source": "# Stage 3 : Test Different Models and parameters"
    },
    {
      "metadata": {
        "_cell_guid": "71a2891c-78fc-443c-a622-e28ffe79c465",
        "_uuid": "1fa53eac6aa18e50979c27c38b26b75287ed7630"
      },
      "cell_type": "markdown",
      "source": "## Slit data into test and training"
    },
    {
      "metadata": {
        "_cell_guid": "73f4510e-9e33-48b7-96f4-08842da93099",
        "_uuid": "1036293295a5a355f3768ad7fa88457ff8122320",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nNUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Sex_female','Sex_male','Title_Master', 'Title_Miss',\n       'Title_Mr', 'Title_Mrs', 'Title_Services','Embarked']\n\n# create test and training data\npredictors = df_train.drop(['Survived', 'PassengerId'], axis=1)\ndata_to_train = df_train[NUMERIC_COLUMNS].fillna(-1000)\ntarget = df_train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(data_to_train, y, test_size = 0.3, random_state = 0)\n\n# create test and training data\n#data_to_train = df_train[NUMERIC_COLUMNS].fillna(-1000)\n#y=df_train['Survived']\n#X=data_to_train\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=21, stratify=y)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f62f192a-36a1-4871-8f44-3e00b44d5979",
        "_uuid": "bc35be0147dd8a4382df02b47aefb12212ab7202"
      },
      "cell_type": "markdown",
      "source": "## Support Vector Classification\n\nHas more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\nThis class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme."
    },
    {
      "metadata": {
        "_cell_guid": "e3eb9069-69a1-4b79-82dd-d03e1dcc38af",
        "collapsed": true,
        "_uuid": "fcaa5d78fcf221bc3a2019c9661230a130952fe2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "clf = SVC()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(x_val)\nacc_clf = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_clf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "85b47396-750a-4a35-b415-acea360679d0",
        "_uuid": "f0150561749e08696a0c150fba42f439a1c07f3e"
      },
      "cell_type": "markdown",
      "source": "## Naive Bayes"
    },
    {
      "metadata": {
        "_cell_guid": "6b1a74a3-2834-47ff-aee0-b2bfc68540fc",
        "collapsed": true,
        "_uuid": "d24400ad0fac6c28dddd9b6f0b71ceba8a39e653",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "035469fb-4b8d-4d2d-a056-4498f7d4e815",
        "_uuid": "4f5be1e8410b2208897ae8df93f334ee63332805"
      },
      "cell_type": "markdown",
      "source": "## Decision Tree"
    },
    {
      "metadata": {
        "_cell_guid": "69e9aa91-4451-47ae-95db-e9c8da605311",
        "collapsed": true,
        "_uuid": "21cd723ecb8e0b3c517aa7681523ddde3dab0cd8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "074abaa3-522d-43c7-a979-c370d52ae47f",
        "_uuid": "763494c08e2edddef19b49608da458774cb8515f"
      },
      "cell_type": "markdown",
      "source": "## Random Forest"
    },
    {
      "metadata": {
        "_cell_guid": "104a417c-d913-40fe-81d7-97f71a8db89a",
        "collapsed": true,
        "_uuid": "790e9ad10cc6cd242fa959ebec9cc63b9a485996",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "88a917f8-e75d-48e8-af76-5ee7da57f9dd",
        "_uuid": "64b2374dd1d3ffab3b63085cd9a8dcb9b01b3eec",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0a856f73-d97c-4fee-ab24-f96bf1463515",
        "_uuid": "7928f2bbaa15f125bbabb28487f3e7ee42976212"
      },
      "cell_type": "markdown",
      "source": "# Reforcast predictions based on performing model"
    },
    {
      "metadata": {
        "_cell_guid": "709f0379-c657-4f14-935b-b9911e23314f",
        "collapsed": true,
        "_uuid": "f10a07d93fb907af3eb3d7876309df62e98119ec",
        "trusted": false
      },
      "cell_type": "code",
      "source": "test = df_test[NUMERIC_COLUMNS].fillna(-1000)\n\nSubmission['Survived']=decisiontree.predict(test)\nprint(Submission.head(5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "82290831-7907-43ef-85ba-f81144ccf66b",
        "_uuid": "6576b795b79a149b2558245570962f97ad4efb7b"
      },
      "cell_type": "markdown",
      "source": "# Make final submission"
    },
    {
      "metadata": {
        "_cell_guid": "f9deff28-86bf-42a6-b54f-ccfe96448f0e",
        "collapsed": true,
        "_uuid": "a10c646954ca4b44fb2c7a4c8f214e6562faf540",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\n#Submission.set_index('PassengerId', inplace=True)\n#Submission.to_csv('finalsubmission.csv',sep=',')\nSubmission.to_csv('decisiontreesubmission.csv',sep=',')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ee85a578-c963-42d8-ac8f-0097737527ab",
        "_uuid": "2cbd72ea1657a90ba05e837969cac1f4d8530bb0"
      },
      "cell_type": "markdown",
      "source": "# Credit where credits due\n\nThis competition is predominantly a training exercise and as such I have tried to looks at different approaches and try different techniques to see hw they work.  I have looked at some of the existing entries and adopted some of the tequiques that i have found interesting. So firstly a huge thanks to everyone that look the time to document their code and explain step by step what they did and why.\n\nTo naming names, some of the notebooks that i found most useful and think deserve special mensions are:\n\n### Anisotropic\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook\n\nIntroduction to Ensembling/Stacking in Python is a very useful project on many levels, in particular I borrowed the pairplot idea for visualising the data.\n\n### Henrique Mello \nhttps://www.kaggle.com/hrmello/introduction-to-data-exploration-using-seaborn/notebook\n\nThis was very helpful in getting title data while feature engineering, I also used some code for to Visualisation new features using FacetGrid from seaborn.\n\n### Omar El Gabry\nhttps://www.kaggle.com/omarelgabry/a-journey-through-titanic?scriptVersionId=447802/notebook\n\nThis kernal has an interesting section on estimating the missing ages and calculating pearson co-efficients for the features.\n\n### Nadin Tamer\nhttps://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner/notebook\n\nI found this another really useful kernel. It is very much a step by step approach, with a particularly good section on different types of model and how they perform for this project."
    },
    {
      "metadata": {
        "_cell_guid": "1ccfbf62-7acc-41c2-9d48-b312c3585517",
        "_uuid": "f7784ed6d938ae46f8f27e3c974fb8222765b34d",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Summary\n\nIn this project we have explored the Titanic Data Set, we have identified missing data and filled then as best we could, we have converted categorical data to columns of numeric features that we can use in machine learning and we have engineered new features based on the data we had. We improved our score from base line of 0.57894 to  a score of 0.75598.\n\nWe  looked at a range of different models and compared the accuracy of each model on the training data to decide which model to use. We then produced predictions from the best performing models which we submitted to ensure that our models were not overfitting. \n\nWe certainly didn't come any where near winning this contest,, but we survived our first Kaggle competition, and hopefully we had fun and learnt alot along the way by looking at what other people were doing and trying different techniques."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}